{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "the project goal is to use the Immigration data and temperature data to enable the analyst to query the compained data in order to be able to determine if the temperature affect the behavior of the Immigration.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "i will use spark to process the Immigration data and the temperature data in order to aggregate the two datasets by the city to get two dimensions table, then i will aggregate the two dimensions table to create the fact table.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### I94 Immigration Data: \n",
    "* This data comes from the US National Tourism and Trade Office. You can read more about it here https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "* It is in SAS7BDAT format.\n",
    "\n",
    "##### Notes:\n",
    "\n",
    "* i94YR = 4 digit year.\n",
    "* i94MON = numeric month.\n",
    "* i94CIT = 3 digit code of origin city.\n",
    "* i94PORT = 3 character code of destination USA city.\n",
    "* ARRDATE = arrival date in the USA.\n",
    "* i94MODE = 1 digit travel code.\n",
    "* DEPDATE = departure date from the USA.\n",
    "* i94VISA = immigration reason.\n",
    "\n",
    "\n",
    "##### World Temperature Data: \n",
    "* This dataset came from Kaggle. You can read more about it here https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "* It is in csv format.\n",
    "\n",
    "##### Notes:\n",
    "* AverageTemperature = average temperature.\n",
    "* City = city name.\n",
    "* Country = country name,.\n",
    "* Latitude= latitude.\n",
    "* Longitude = longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read feb data\n",
    "df_immigration=spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat')\n",
    "df_immigration.createOrReplaceTempView(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read the temperature data\n",
    "df_temperature = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "* invalid i94PORT\n",
    "* NaN AverageTemperature\n",
    "* duplicated locations\n",
    "\n",
    "#### Cleaning Steps\n",
    "##### immigration data: \n",
    "* remove the rows that contains invalid i94PORT as mentioned in I94_SAS_Labels_Description file.\n",
    "\n",
    "##### Temperature Data:\n",
    "* remove the rows that contains NaN AverageTemperature or duplicated locations. \n",
    "* adding the I94PORT of the location in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of valid ports\n",
    "re_file = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "validPorts = {}\n",
    "with open('validports.txt') as file:\n",
    "     for port in file:\n",
    "         match = re_file.search(port)\n",
    "         validPorts[match[1]]=[match[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|      admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|  2.0|2016.0|   2.0| 101.0| 101.0|    ATL|20498.0|    1.0|     MI|   null|  21.0|    3.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1995.0|     D/S|     F|  null|     DL|4.91319785E8|  241|      F1|\n",
      "|  5.0|2016.0|   2.0| 101.0| 101.0|    CHI|20492.0|    1.0|     IL|   null|  55.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1961.0|08072016|     F|  null|     TK|4.70581085E8|    5|      B2|\n",
      "|  6.0|2016.0|   2.0| 101.0| 101.0|    CHI|20492.0|    1.0|     IL|   null|   6.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 2010.0|08072016|     M|  null|     TK|4.70572885E8|    5|      B2|\n",
      "|  7.0|2016.0|   2.0| 101.0| 101.0|    CHI|20500.0|    1.0|     AZ|20527.0|  38.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1978.0|08152016|     F|  null|     LH|4.97400985E8|  434|      B2|\n",
      "|  8.0|2016.0|   2.0| 101.0| 101.0|    CHI|20503.0|    1.0|     IL|20518.0|  37.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1979.0|08182016|     M|  null|     AA|5.07772085E8|   87|      B2|\n",
      "|  9.0|2016.0|   2.0| 101.0| 101.0|    CHI|20506.0|    1.0|     IL|20520.0|  21.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1995.0|08212016|     M|  null|     OS|5.23079485E8|   65|      B2|\n",
      "| 13.0|2016.0|   2.0| 101.0| 101.0|    TUC|20495.0|    1.0|     MI|   null|  43.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1973.0|08102016|     M|  null|     LH|4.78972185E8|  442|      B2|\n",
      "| 15.0|2016.0|   2.0| 101.0| 101.0|    TUC|20501.0|    1.0|     MI|   null|  20.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1996.0|08162016|     M|  null|     DL|5.02626885E8|  137|      B2|\n",
      "| 16.0|2016.0|   2.0| 101.0| 101.0|    TUC|20503.0|    1.0|   null|20519.0|  42.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1974.0|08182016|     F|  null|     LH|5.09809885E8|  442|      B2|\n",
      "| 17.0|2016.0|   2.0| 101.0| 101.0|    TUC|20504.0|    1.0|     MI|   null|  72.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1944.0|08192016|     F|  null|     LH|5.13194085E8|  442|      B2|\n",
      "| 18.0|2016.0|   2.0| 101.0| 101.0|    TUC|20504.0|    1.0|     MI|   null|  54.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1962.0|08192016|     M|  null|     LH|5.13186885E8|  442|      B2|\n",
      "| 19.0|2016.0|   2.0| 101.0| 101.0|    TUC|20504.0|    1.0|     MI|   null|  27.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1989.0|08192016|     M|  null|     LH|5.13195085E8|  442|      B2|\n",
      "| 20.0|2016.0|   2.0| 101.0| 101.0|    LOS|20495.0|    1.0|     FL|   null|  23.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1993.0|08102016|     M|  null|     QF|4.77186585E8|   93|      B2|\n",
      "| 23.0|2016.0|   2.0| 101.0| 101.0|    NEW|20495.0|    1.0|     NY|   null|  34.0|    1.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1982.0|08102016|     M|  null|     LH|4.79654185E8|  412|      B1|\n",
      "| 24.0|2016.0|   2.0| 101.0| 101.0|    NEW|20503.0|    1.0|     NJ|20513.0|  23.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1993.0|08182016|     M|  null|     OS|5.07867385E8|   89|      B2|\n",
      "| 26.0|2016.0|   2.0| 101.0| 101.0|    NYC|20491.0|    1.0|     NY|   null|  77.0|    2.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1939.0|08062016|     M|  null|     OS|4.64868385E8|   87|      B2|\n",
      "| 29.0|2016.0|   2.0| 101.0| 101.0|    NYC|20497.0|    1.0|     NY|20507.0|  51.0|    1.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1965.0|08122016|     M|  null|     TK|4.85192785E8|    3|      B1|\n",
      "| 32.0|2016.0|   2.0| 101.0| 101.0|    NYC|20506.0|    1.0|     NY|20534.0|  27.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1989.0|08212016|     F|  null|     AZ|5.23095085E8|  610|      B2|\n",
      "| 33.0|2016.0|   2.0| 101.0| 101.0|    NYC|20506.0|    1.0|     NY|20535.0|  36.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1980.0|08212016|     F|  null|     AZ|5.23058385E8|  610|      B2|\n",
      "| 36.0|2016.0|   2.0| 101.0| 101.0|    TAM|20490.0|    1.0|     FL|20590.0|  55.0|    2.0|  1.0|    null|    null| null|      T|      O|   null|      M| 1961.0|08052016|     F|  null|     LH|4.62174285E8|  482|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# remove the rows where i94port is invalid\n",
    "df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(validPorts.keys())))\n",
    "\n",
    "df_immigration.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+---------+\n",
      "| AverageTemperature| City|Country|Latitude|Longitude|\n",
      "+-------------------+-----+-------+--------+---------+\n",
      "| 16.500999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 12.947000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              6.549|Århus|Denmark|  57.05N|   10.33E|\n",
      "|0.40299999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             -1.492|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              1.431|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             10.675|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             15.919|Århus|Denmark|  57.05N|   10.33E|\n",
      "|               9.88|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             16.459|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              12.05|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             10.378|Århus|Denmark|  57.05N|   10.33E|\n",
      "|                2.4|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              4.382|Århus|Denmark|  57.05N|   10.33E|\n",
      "|             18.651|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              5.518|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              0.113|Århus|Denmark|  57.05N|   10.33E|\n",
      "|              14.57|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 14.915999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 15.916999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature = spark.sql(\"\"\"\n",
    "                            select distinct temperature.AverageTemperature, temperature.City, \n",
    "                            temperature.Country, temperature.Latitude, temperature.Longitude \n",
    "                            from temperature  \n",
    "                            where temperature.AverageTemperature !='NaN'  \"\"\")\n",
    "\n",
    "#df_temperature = df_temperature.filter(df_temperature.AverageTemperature != 'NaN')\n",
    "df_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def get_city(city):\n",
    "\n",
    "    for key in validPorts:\n",
    "        if city.lower() in validPorts[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+---------+-------+\n",
      "| AverageTemperature| City|Country|Latitude|Longitude|i94port|\n",
      "+-------------------+-----+-------+--------+---------+-------+\n",
      "| 16.500999999999998|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "| 12.947000000000001|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              6.549|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|0.40299999999999997|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             -1.492|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              1.431|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             10.675|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             15.919|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|               9.88|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             16.459|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              12.05|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             10.378|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|                2.4|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              4.382|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|             18.651|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              5.518|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              0.113|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|              14.57|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "| 14.915999999999999|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "| 15.916999999999998|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "+-------------------+-----+-------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add port code based on city name\n",
    "df_temperature = df_temperature.withColumn(\"i94port\", get_city(df_temperature.City))\n",
    "df_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "#df_temperature = spark.sql(\"\"\"\n",
    "#                            select temperature.AverageTemperature, temperature.City, \n",
    "#                            temperature.Country, temperature.Latitude, temperature.Longitude,temperature.i94port\n",
    "#                            from temperature  \n",
    "#                            where temperature.i94port is not null  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_temperature = df_temperature.filter(df_temperature.i94port != 'null')\n",
    "\n",
    "df_temperature.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "##### Dimension Table - immigration data Columns:\n",
    "* I94YR = 4 digit year.\n",
    "* I94MON = numeric month.\n",
    "* I94CIT = 3 digit code of origin city.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "* ARRDATE = arrival date in the USA.\n",
    "* I94MODE = 1 digit travel code.\n",
    "* DEPDATE = departure date from the USA.\n",
    "* I94VISA = immigration reason.\n",
    "##### Dimension Table - temperature data Columns:\n",
    "* AverageTemperature = average temperature.\n",
    "* City = city name.\n",
    "* Country = country name,.\n",
    "* Latitude= latitude.\n",
    "* Longitude = longitude.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "\n",
    "##### Fact Table - immigration data joined with the temperature Columns:\n",
    "* year = 4 digit year.\n",
    "* month = numeric month.\n",
    "* city = 3 digit code of origin city.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "* arrival_date = arrival date in the USA.\n",
    "* travel_code = 1 digit travel code.\n",
    "* departure_date = departure date from the USA.\n",
    "* reason = immigration reason.\n",
    "* AverageTemperature = average temperature of destination city,\n",
    "* Latitude= latitude.\n",
    "* Longitude = longitude.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "<img src=\"PIPline.png\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration=spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read the temperature data\n",
    "df_temperature = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove the rows where i94port is invalid\n",
    "df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(validPorts.keys())))\n",
    "df_immigration.createOrReplaceTempView(\"immigration\")\n",
    "immigration = spark.sql(\"\"\"\n",
    "                            select i94yr, i94mon, i94cit, i94port, arrdate, i94mode, depdate, i94visa\n",
    "                            from immigration  \n",
    "                              \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#remove the rows that contains NaN AverageTemperature or duplicated locations.\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "temperature = spark.sql(\"\"\"\n",
    "                            select distinct temperature.AverageTemperature, temperature.City, \n",
    "                            temperature.Country, temperature.Latitude, temperature.Longitude\n",
    "                            from temperature  \n",
    "                            where temperature.AverageTemperature !='NaN'  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature = temperature.withColumn(\"i94port\", get_city(temperature.City))\n",
    "temperature = temperature.filter(temperature.i94port != 'null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write immigration dimension table.\n",
    "immigration.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/immigration/immigration.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write temperature dimension table.\n",
    "temperature.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/temperature/temperature.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.createOrReplaceTempView(\"immigration\")\n",
    "temperature.createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table = spark.sql('''\n",
    "select immigration.i94yr as year,\n",
    "       immigration.i94mon as month,\n",
    "       immigration.i94cit as city,\n",
    "       immigration.i94port as i94port,\n",
    "       immigration.arrdate as arrival_date,\n",
    "       immigration.i94mode as travel_code,\n",
    "       immigration.depdate as departure_date,\n",
    "       immigration.i94visa as reason,\n",
    "       immigration.AverageTemperature as temperature,\n",
    "       immigration.Latitude as latitude,\n",
    "       immigration.Longitude as longitude\n",
    "from immigration\n",
    "JOIN temperature ON (immigration.i94port = temperature.i94port)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/fact/fact.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_checks(data, table):\n",
    "    \n",
    "    if data.count() == 0:\n",
    "        print(\"Data quality failed for {} table with 0 events\".format(table))\n",
    "    else:\n",
    "        print(\"Data quality passed for {} table with {} events\".format(description, data.count()))\n",
    "    return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "quality_checks(immigration, \"immigration\")\n",
    "quality_checks(temperature, \"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "##### Dimension Table - immigration data Columns:\n",
    "* I94YR = 4 digit year.\n",
    "* I94MON = numeric month.\n",
    "* I94CIT = 3 digit code of origin city.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "* ARRDATE = arrival date in the USA.\n",
    "* I94MODE = 1 digit travel code.\n",
    "* DEPDATE = departure date from the USA.\n",
    "* I94VISA = immigration reason.\n",
    "##### Dimension Table - temperature data Columns:\n",
    "* AverageTemperature = average temperature.\n",
    "* City = city name.\n",
    "* Country = country name,.\n",
    "* Latitude= latitude.\n",
    "* Longitude = longitude.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "##### Fact Table - immigration data joined with the temperature Columns:\n",
    "* year = 4 digit year.\n",
    "* month = numeric month.\n",
    "* city = 3 digit code of origin city.\n",
    "* I94PORT = 3 character code of destination USA city.\n",
    "* arrival_date = arrival date in the USA.\n",
    "* travel_code = 1 digit travel code.\n",
    "* departure_date = departure date from the USA.\n",
    "* reason = immigration reason.\n",
    "* AverageTemperature = average temperature of destination city,\n",
    "* Latitude= latitude.\n",
    "* Longitude = longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* i have chosen spark in order if there is a need for scaling up in the future\n",
    "* i have chosen parquet format because it's columnar format and its compatible with hdfs if there is a need for scaling up in the future i could use hdfs\n",
    "##### Propose how often the data should be updated and why.\n",
    "* each month because the data is provided in monthly files.\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * i will add more spark nodes to scale up.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * i will use orchestration tool like Airflow or Luigi to run the pipline every day if needed or i will use simple cron job depend on the situation  \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * if it's not able to handle the number of user i may change the node profile or i will use a big data technolagy like hdfs or S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
