{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red35\green46\blue57;\red255\green255\blue255;\red62\green62\blue62;
}
{\*\expandedcolortbl;;\cssrgb\c18039\c23922\c28627;\cssrgb\c100000\c100000\c100000;\cssrgb\c30980\c30980\c30980;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl480\sa100\partightenfactor0

\f0\b\fs36 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Differences between HDFS and AWS S3\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0\fs32 \cf4 \strokec4 Since Spark does not have its own distributed storage system, it leverages using HDFS or AWS S3, or any other distributed storage. Primarily in this course, we will be using AWS S3, but let\'92s review the advantages of using HDFS over AWS S3.\
Although it would make the most sense to use AWS S3 while using other AWS services, it\'92s important to note the differences between AWS S3 and HDFS.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls1\ilvl0
\f0\b\fs24 \cf4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 AWS S3
\f1\b0 \'a0is an\'a0
\f0\b object storage system
\f1\b0 \'a0that stores the data using key value pairs, namely bucket and key, and\'a0
\f0\b HDFS
\f1\b0 \'a0is an\'a0
\f0\b actual distributed file system
\f1\b0 \'a0which guarantees fault tolerance. HDFS achieves fault tolerance by having duplicate factors, which means it will duplicate the same files at 3 different nodes across the cluster by default (it can be configured to different numbers of duplication).\cb1 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 HDFS has usually been\'a0
\f0\b installed in on-premise systems
\f1\b0 , and traditionally have had engineers on-site to maintain and troubleshoot Hadoop Ecosystem, which\'a0
\f0\b cost more than having data on cloud
\f1\b0 . Due to the\'a0
\f0\b flexibility of location
\f1\b0 \'a0and\'a0
\f0\b reduced cost of maintenance
\f1\b0 , cloud solutions have been more popular. With extensive services you can use within AWS, S3 has been a more popular choice than HDFS.\cb1 \uc0\u8232 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Since\'a0
\f0\b AWS S3 is a binary object store
\f1\b0 , it can\'a0
\f0\b store all kinds of format
\f1\b0 , even images and videos. HDFS will strictly require a certain file format - the popular choices are\'a0
\f0\b avro
\f1\b0 \'a0and\'a0
\f0\b parquet
\f1\b0 , which have relatively high compression rate and which makes it useful to store large dataset.\cb1 \uc0\u8232 \
}