{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;\red35\green46\blue57;\red255\green255\blue255;\red62\green62\blue62;
\red14\green32\blue46;\red245\green245\blue246;\red21\green163\blue221;}
{\*\expandedcolortbl;;\cssrgb\c18039\c23922\c28627;\cssrgb\c100000\c100000\c100000;\cssrgb\c30980\c30980\c30980;
\cssrgb\c5882\c16863\c23922;\cssrgb\c96863\c96863\c97255;\cssrgb\c784\c70196\c89412;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl480\sa100\partightenfactor0

\f0\b\fs36 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Functions\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0\fs32 \cf4 \strokec4 In the previous video, we've used a number of functions to manipulate our dataframe. Let's take a look at the different type of functions and their potential pitfalls.\
\pard\pardeftab720\sl426\sa100\partightenfactor0

\f0\b \cf2 \strokec2 General functions\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0 \cf4 \strokec4 We have used the following general functions that are quite similar to methods of pandas dataframes:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl480\partightenfactor0
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 select()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : returns a new DataFrame with the selected columns\cb1 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 filter()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : filters rows using the given condition\cb1 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 where()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : is just an alias for\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 filter()
\f1\fs24 \cf4 \cb1 \strokec4 \uc0\u8236 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 groupBy()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : groups the DataFrame using the specified columns, so we can run aggregation on them\cb1 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 sort()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : returns a new DataFrame sorted by the specified column(s). By default the second parameter 'ascending' is True.\cb1 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 dropDuplicates()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : returns a new DataFrame with unique rows based on all or just a subset of columns\cb1 \
\ls1\ilvl0
\f2\fs28\fsmilli14400 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 \uc0\u8237 withColumn()
\f1\fs24 \cf4 \cb3 \strokec4 \uc0\u8236 : returns a new DataFrame by adding a column or replacing the existing column that has the same name. The first parameter is the name of the new column, the second is an expression of how to compute it.\cb1 \
\pard\pardeftab720\sl426\sa100\partightenfactor0

\f0\b\fs32 \cf2 \cb3 \strokec2 Aggregate functions\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0 \cf4 \strokec4 Spark SQL provides built-in methods for the most common aggregations such as\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 count()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 ,\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 countDistinct()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 ,\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 avg()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 ,\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 max()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 ,\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 min()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 , etc. in the pyspark.sql.functions module. These methods are not the same as the built-in methods in the Python Standard Library, where we can find\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 min()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 \'a0for example as well, hence you need to be careful not to use them interchangeably.\
In many cases, there are multiple ways to express the same aggregations. For example, if we would like to compute one type of aggregate for one or more columns of the DataFrame we can just simply chain the aggregate method after a\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 groupBy()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 . If we would like to use different functions on different columns,\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 agg()
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 comes in handy. For example\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 agg(\{"salary": "avg", "age": "max"\})
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 \'a0computes the average salary and maximum age.\
\pard\pardeftab720\sl426\sa100\partightenfactor0

\f0\b \cf2 \strokec2 User defined functions (UDF)\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0 \cf4 \strokec4 In Spark SQL we can define our own functions with the udf method from the pyspark.sql.functions module. The default type of the returned variable for UDFs is string. If we would like to return an other type we need to explicitly do so by using the different types from the pyspark.sql.types module.\
\pard\pardeftab720\sl426\sa100\partightenfactor0

\f0\b \cf2 \strokec2 Window functions\
\pard\pardeftab720\sl540\sa300\partightenfactor0

\f1\b0 \cf4 \strokec4 Window functions are a way of combining the values of ranges of rows in a DataFrame. When defining the window we can choose how to sort and group (with the\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 partitionBy
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 \'a0method) the rows and how wide of a window we'd like to use (described by\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 rangeBetween
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 \'a0or\'a0
\f2\fs28\fsmilli14400 \cf5 \cb6 \strokec5 \uc0\u8237 rowsBetween
\f1\fs32 \cf4 \cb3 \strokec4 \uc0\u8236 ).\
\pard\pardeftab720\sl540\partightenfactor0
\cf4 For further information see the\'a0{\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/sql-programming-guide.html"}}{\fldrslt 
\f0\b \cf7 \strokec7 Spark SQL, DataFrames and Datasets Guide}}\'a0and the\'a0{\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/api/python/index.html"}}{\fldrslt 
\f0\b \cf7 \strokec7 Spark Python API Docs}}.\
}